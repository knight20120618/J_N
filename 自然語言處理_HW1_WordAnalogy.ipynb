{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://download.tensorflow.org/data/questions-words.txt\n",
    "# 以上一個步驟在Google Colab執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理資料集\n",
    "with open(\"questions-words.txt\", \"r\") as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "# 檢查前四筆\n",
    "for entry in data[:5]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存以 : 開頭的句子\n",
    "start_line = []\n",
    "# 建立空LIST\n",
    "questions = []\n",
    "categories = []\n",
    "sub_categories = []\n",
    "\n",
    "# Question\n",
    "for i in range(len(data)):\n",
    "    if data[i].startswith(\":\"):\n",
    "        # 紀錄行號及 : 後的句子\n",
    "        start_line.append((i, data[i]))\n",
    "    else:\n",
    "        questions.append(data[i])\n",
    "    \n",
    "# Category\n",
    "m = (start_line[5][0] - 5)\n",
    "n = (len(data) - m  - len(start_line))\n",
    "for o in range(0, m):\n",
    "    # 前五筆的\n",
    "    categories.append(\"semantic\")\n",
    "for o in range(0, n):\n",
    "    # 後九筆的\n",
    "    categories.append(\"syntatic\")\n",
    "\n",
    "# SubCategory\n",
    "for j in range(len(start_line)):\n",
    "    # 最後一句 : 的句子\n",
    "    if j == (len(start_line) - 1):\n",
    "        k = (len(data) - len(start_line) - start_line[j][0])\n",
    "        for l in range(0, k):\n",
    "            sub_categories.append(start_line[j][1])\n",
    "    # 其餘的\n",
    "    else:\n",
    "        k = (start_line[j+1][0] - start_line[j][0])\n",
    "        for l in range(0, k):\n",
    "            sub_categories.append(start_line[j][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 建立DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": questions,\n",
    "        \"Category\": categories,\n",
    "        \"SubCategory\": sub_categories,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 顯示前五筆\n",
    "print(df.head())\n",
    "\n",
    "# 檔案存成csv格式\n",
    "df.to_csv(\"questions-words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II: Use pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 載入資料\n",
    "data = pd.read_csv(\"questions-words.csv\")\n",
    "\n",
    "# 載入模型\n",
    "model = gensim.downloader.load(\"glove-wiki-gigaword-100\")\n",
    "print(\"The Gensim model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "for analogy in tqdm(data[\"Question\"]):\n",
    "    # 切割句子\n",
    "    s = analogy.split()\n",
    "    golds.append(s[3])\n",
    "    # 詞向量函數\n",
    "    def w2v(word_a, word_b, word_c, model):\n",
    "        try:\n",
    "            result_vector = model[word_b] + model[word_c] - model[word_a]\n",
    "            closest_word = model.most_similar(positive=[result_vector], topn=1)[0][0]\n",
    "            return closest_word\n",
    "        except KeyError as e:\n",
    "            return None\n",
    "    preds.append(w2v(s[0], s[1], s[2], model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義calculate_accuracy函數\n",
    "def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:\n",
    "    return np.mean(gold == pred)\n",
    "\n",
    "golds_np, preds_np = np.array(golds), np.array(preds)\n",
    "data = pd.read_csv(\"questions-words.csv\")\n",
    "\n",
    "# Evaluation: categories\n",
    "for category in data[\"Category\"].unique():\n",
    "    mask = data[\"Category\"] == category\n",
    "    golds_cat, preds_cat = golds_np[mask], preds_np[mask]\n",
    "    acc_cat = calculate_accuracy(golds_cat, preds_cat)\n",
    "    print(f\"Category: {category}, Accuracy: {acc_cat * 100}%\")\n",
    "\n",
    "# Evaluation: sub-categories\n",
    "for sub_category in data[\"SubCategory\"].unique():\n",
    "    mask = data[\"SubCategory\"] == sub_category\n",
    "    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]\n",
    "    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)\n",
    "    print(f\"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集\n",
    "df = pd.DataFrame(data)\n",
    "# 目標\n",
    "SUB_CATEGORY = \": family\"\n",
    "\n",
    "filtered_df = df[df[\"SubCategory\"] == SUB_CATEGORY]\n",
    "\n",
    "words = []\n",
    "for p in filtered_df[\"Question\"]:\n",
    "    words.append(p.split())\n",
    "\n",
    "# 轉一維LIST及去除重複字詞\n",
    "words = list(set([item for word in words for item in word]))\n",
    "\n",
    "# 文字轉詞向量\n",
    "word_vectors = [model[word] for word in words]\n",
    "\n",
    "# 使用 t-SNE 進行降維\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=10)\n",
    "word_vectors_2d = tsne.fit_transform(np.array(word_vectors))\n",
    "\n",
    "# 繪製圖像\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "# 標記每個點\n",
    "for i, word in enumerate(words):\n",
    "    plt.text(word_vectors_2d[i, 0], word_vectors_2d[i, 1], word, fontsize=10)\n",
    "\n",
    "plt.title(\"Word Relationships from Google Analogy Task\")\n",
    "plt.savefig(\"word_relationships.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part III: Train your own word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz\n",
    "!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz\n",
    "!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz\n",
    "!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz\n",
    "!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz\n",
    "!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz\n",
    "!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz\n",
    "!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz\n",
    "!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz\n",
    "!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz\n",
    "!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz\n",
    "\n",
    "!gunzip -k wiki_texts_part_*.gz\n",
    "!cat wiki_texts_part_*.txt > wiki_texts_combined.txt\n",
    "\n",
    "!head -n 10 wiki_texts_combined.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "path0 = '/content/wiki_texts_combined.txt'\n",
    "path1 = '/content/drive/MyDrive/wiki_texts_combined.txt'\n",
    "\n",
    "shutil.copyfile(path0, path1)\n",
    "print(\"OK\")\n",
    "# 以上三個步驟在Google Colab執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "# 使用壓縮檔\n",
    "with open(\"wiki_texts_combined.txt\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    print(\"#等待3~4分鐘\")\n",
    "    lines = f.readlines()\n",
    "    print(\"#讀取OK\")\n",
    "\n",
    "samples = random.sample(lines, int(len(lines) * 0.2))\n",
    "\n",
    "with open(\"wiki_texts_output.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    print(\"#取得20%\")\n",
    "    for line in samples:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "with open(\"wiki_texts_output.txt\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    samples = f.readlines()\n",
    "#下載數據包\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "#預處理函數\n",
    "def preprocess_texts(text):\n",
    "    for i in range(len(samples)):\n",
    "        #字詞分割\n",
    "        words = word_tokenize(text[i])\n",
    "        #去除標點符號\n",
    "        words = [word for word in words if word not in string.punctuation]\n",
    "        return words\n",
    "\n",
    "print(\"#預處理\")\n",
    "corpus = [preprocess_texts(samples)]\n",
    "\n",
    "print(\"#訓練開始\")\n",
    "model = Word2Vec(sentences=corpus, min_count=1, workers=4)\n",
    "\n",
    "print(\"#儲存\")\n",
    "model.save(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from tqdm import  tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# 載入模型\n",
    "model = Word2Vec.load(\"w2v.model\")\n",
    "# 載入資料\n",
    "data = pd.read_csv(\"questions-words.csv\")\n",
    "\n",
    "preds = []\n",
    "golds = []\n",
    "for analogy in tqdm(data[\"Question\"]):\n",
    "    # 切割句子\n",
    "    s = analogy.split()\n",
    "    golds.append(s[3])\n",
    "    # 詞向量函數\n",
    "    def w2v(word_a, word_b, word_c, model):\n",
    "        try:\n",
    "            result_vector = model.wv[word_b] + model.wv[word_c] - model.wv[word_a]\n",
    "            closest_word = model.wv.most_similar(positive=[result_vector], topn=1)[0][0]\n",
    "            return closest_word\n",
    "        except KeyError:\n",
    "            return None\n",
    "    preds.append(w2v(s[0], s[1], s[2], model))\n",
    "\n",
    "# 資料集\n",
    "df = pd.DataFrame(data)\n",
    "# 目標\n",
    "SUB_CATEGORY = \": family\"\n",
    "\n",
    "filtered_df = df[df[\"SubCategory\"] == SUB_CATEGORY]\n",
    "\n",
    "words = []\n",
    "for p in filtered_df[\"Question\"]:\n",
    "    words.append(p.split())\n",
    "\n",
    "# 轉一維LIST及去除重複字詞\n",
    "words = list(set([item for word in words for item in word]))\n",
    "\n",
    "# 文字轉詞向量\n",
    "word_vectors = []\n",
    "for word in words:\n",
    "    try:\n",
    "        word_vector = model.wv[word]\n",
    "    except KeyError:\n",
    "        # 找到與缺失詞語最相似的詞語\n",
    "        all_words = model.wv.index_to_key  # 所有詞彙\n",
    "        word_similarities = []\n",
    "        \n",
    "        random_vector = np.random.rand(model.vector_size)\n",
    "        \n",
    "        # 計算與模型中的每個詞語的餘弦相似度\n",
    "        for candidate_word in all_words:\n",
    "            candidate_vector = model.wv[candidate_word]\n",
    "            # 使用餘弦相似度公式\n",
    "            similarity = np.dot(candidate_vector, random_vector) / (np.linalg.norm(candidate_vector) * np.linalg.norm(random_vector))\n",
    "            word_similarities.append((candidate_word, similarity))\n",
    "\n",
    "        # 找到相似度最高的詞\n",
    "        most_similar_word = sorted(word_similarities, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        word_vector = model.wv[most_similar_word]\n",
    "    \n",
    "    word_vectors.append(word_vector)\n",
    "\n",
    "# 使用 t-SNE 進行降維\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=10)\n",
    "word_vectors_2d = tsne.fit_transform(np.array(word_vectors))\n",
    "\n",
    "# 繪製圖像\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "# 標記每個點\n",
    "for i, word in enumerate(words):\n",
    "    plt.text(word_vectors_2d[i, 0], word_vectors_2d[i, 1], word, fontsize=10)\n",
    "\n",
    "plt.title(\"Word Relationships from Google Analogy Task\")\n",
    "plt.savefig(\"w2v.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
